<!DOCTYPE html>

<html lang="en">
<head>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GZ3LNX2W9L"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GZ3LNX2W9L');
</script>
<link href="http://gmpg.org/xfn/11" rel="profile"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<!-- Enable responsiveness on mobile devices-->
<meta content="width=device-width, initial-scale=1.0, maximum-scale=1" name="viewport"/>
<title>Research group - Luiz Chamon</title>
<!-- CSS -->
<link href="https://www.luizchamon.com/theme/css/poole.css" rel="stylesheet">
<link href="https://www.luizchamon.com/theme/css/hyde.css" rel="stylesheet"/>
<link href="https://www.luizchamon.com/theme/css/syntax/friendly.css" rel="stylesheet"/>
<!-- FONTS -->
<link href="https://fonts.googleapis.com/" rel="dns-prefetch"/>
<link href="https://fonts.googleapis.com/css?family=Abril+Fatface:700|PT+Sans:400,400italic,700&amp;subset=latin,latin-ext&amp;display=swap" rel="stylesheet"/>
<link href="https://www.luizchamon.com/theme/css/academicons.min.css" rel="stylesheet">
<!-- FAVICON -->
<link href="https://www.luizchamon.com/theme/icon/favicon-32.png" rel="icon" sizes="32x32"/>
<link href="https://www.luizchamon.com/theme/icon/favicon-128.png" rel="icon" sizes="128x128"/>
<link href="https://www.luizchamon.com/theme/icon/favicon-180.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="https://www.luizchamon.com/theme/icon/favicon-192.png" rel="icon" sizes="192x192"/>
</link></link></head>
<body id="">
<div class="menu-toggle">
<a href="#mobile-navigation" id="mobile-menu-open"><span class="fa fa-bars"></span></a>
</div>
<div class="sidebar">
<div class="menu-toggle-sidebar">
<a href="#mobile-navigation" id="mobile-menu-open-alt"><span class="fa fa-bars"></span></a>
</div>
<nav class="sidebar-nav">
<a class="sidebar-nav-item" href="https://www.luizchamon.com/index.html">Home</a>
<a class="sidebar-nav-item" href="https://www.luizchamon.com/pages/about.html">About</a>
<a class="sidebar-nav-item active" href="https://www.luizchamon.com/pages/research-group.html">Research group</a>
<a class="sidebar-nav-item" href="https://www.luizchamon.com/pages/publications.html">Publications</a>
<a class="sidebar-nav-item" href="https://www.luizchamon.com/pages/contact.html">Contact</a>
<a class="sidebar-nav-item" href="https://www.luizchamon.com/pages/prospective-members.html">Prospective members</a>
</nav>
<div class="container sidebar-sticky">
<div class="sidebar-about">
<h1>
<a href="https://www.luizchamon.com">
<img class="profile-picture" src="https://www.luizchamon.com/images/lfochamon_thumb.jpg"/>
					Luiz Chamon
				</a>
</h1>
</div>
<nav class="sidebar-social">
<a class="sidebar-social-item" href="mailto:luiz.chamon@polytechnique.edu">
<i class="fas fa-envelope"></i>
</a>
<a class="sidebar-social-item" href="https://www.luizchamon.com/pdf/lfochamon_cv.pdf">
<i class="ai ai-cv" style="font-size:1rem;"></i>
</a>
<a class="sidebar-social-item" href="https://scholar.google.ca/citations?user=FIm-l-sAAAAJ&amp;hl=en" noreferrer”="" rel="”noopener">
<i class="ai ai-google-scholar" style="font-size:1rem;"></i>
</a>
<a class="sidebar-social-item" href="https://github.com/lfochamon/" noreferrer”="" rel="”noopener">
<i class="fab fa-github"></i>
</a>
<a class="sidebar-social-item" href="https://www.youtube.com/@luizfochamon" noreferrer”="" rel="”noopener">
<i class="fab fa-youtube"></i>
</a>
<!-- <a class="sidebar-nav-item" href="">
				<i class="fa fa-feed"></i>
			</a> -->
</nav>
<p class="copyright">© 2020–2025. All rights reserved.</p>
</div>
</div>
<nav class="mobile-navigation" id="mobile-navigation">
<ul class="primary-menu">
<li><a class="sidebar-nav-item" href="https://www.luizchamon.com/index.html">Home</a></li>
<li><a class="sidebar-nav-item" href="https://www.luizchamon.com/pages/about.html">About</a></li>
<li><a class="sidebar-nav-item active" href="https://www.luizchamon.com/pages/research-group.html">Research group</a></li>
<li><a class="sidebar-nav-item" href="https://www.luizchamon.com/pages/publications.html">Publications</a></li>
<li><a class="sidebar-nav-item" href="https://www.luizchamon.com/pages/contact.html">Contact</a></li>
<li><a class="sidebar-nav-item" href="https://www.luizchamon.com/pages/prospective-members.html">Prospective members</a></li>
</ul>
<a class="menu-close" href="#mobile-navigation-toggle" id="menu-close"><span class="fa fa-close"></span></a>
</nav>
<a class="backdrop" href="#mobile-navigation-toggle" id="backdrop" tabindex="-1"></a> <div class="content container">
<style>
/* Style the button that is used to open and close the collapsible content */
.collapsible button {
  background-color: #555;
  color: white;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: none;
  text-align: left;
  outline: none;
  font-size: 15px;
}

.collapsible p {
  margin: 0;
}

/* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
.active button, .collapsible:hover button {
  background-color: #0a3050;
}

/* Style the collapsible content. Note: hidden by default */
.collapsible-content {
  padding-right: 1rem;
  padding-top: 1rem;
  display: none;
  overflow: hidden;
  background-color: #f1f1f1;
}

.pubs_list{
  padding: 0 30px;
  font-size: 16px;
  list-style-type: none;
}

.pubs_list li{
  margin: 0 0 15px 0;
}

.research-icon {
  margin-top: 2rem;
  margin-bottom: 2rem;
}

@media (min-width: 1400px) {
  .research-item {
    display: flex;
    align-items: center;
  }
  .research-icon {
    flex-shrink: 0;
    order: 2;
    margin-left: 2rem;
  }
}
</style>
<h1 id="research-group">Research group</h1>
<p>My <a href="#people">group</a> develops and analyzes tools that enable intelligent systems to <strong>extract</strong>, <strong>process</strong>, and <strong>act</strong> on information, i.e.,</p>
<ul>
<li><strong>gather data</strong>,
  <span style="color:#999;font-size:90%;">e.g.,
    using sampling<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon18g">1</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong> and A. Ribeiro.
Greedy sampling of graph signals.
<em>IEEE Trans. on Signal Process.</em>, 66[1]:34–47, 2018.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon21a">2</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong>, G. J. Pappas, and A. Ribeiro.
Approximate supermodularity of <span class="bibtex-protected">K</span>alman filter sensor selection.
<em>IEEE Trans. on Autom. Control.</em>, 66[1]:49–63, 2021.</span>
</span>]</sup> and active learning<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon17a">3</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong> and A. Ribeiro.
Approximate supermodularity bounds for experimental design.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 5403–5412. 2017.</span>
</span>]</sup> techniques,
  </span></li>
<li><strong>turn this data into insights</strong>,
  <span style="color:#999;font-size:90%;">e.g.,
    using ML<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon20p">4</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong> and A. Ribeiro.
Probably approximately correct constrained learning.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>. 2020.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Robey21a">5</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">A. Robey*, <strong>L. F. O. Chamon</strong>*, G. J. Pappas, H. Hassani, and A. Ribeiro.
Adversarial robustness with semi-infinite constrained learning.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>. 2021.
<span class="bibtex-protected">(* equal contribution)</span>.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Robey22p">6</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">A. Robey, <strong>L. F. O. Chamon</strong>, G. J. Pappas, and H. Hassani.
Probabilistically robust learning: <span class="bibtex-protected">B</span>alancing average- and worst-case performance.
In <em>International Conference on Machine Learning (ICML)</em>. 2022.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon23c">7</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong>, S. Paternain, M. <span class="bibtex-protected">Calvo-Fullana</span>, and A. Ribeiro.
Constrained learning with non-convex losses.
<em>IEEE Trans. on Inf. Theory</em>, 69[3]:1739–1760, 2023.</span>
</span>]</sup>,
    geometric ML<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Ruiz20g">8</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">L. Ruiz, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
Graphon neural networks and the transferability of graph neural networks.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>. 2020.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Ruiz21g">9</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">L. Ruiz, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
Graphon signal processing.
<em>IEEE Trans. on Signal Process.</em>, 69:4961–4976, 2021.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Ruiz23t">10</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">L. Ruiz, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
Transferability properties of graph neural networks.
<em>IEEE Trans. on Signal Process.</em>, 71:3474–3489, 2023.</span>
</span>]</sup>,
    and statistical methods<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon20f">11</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong>, Y. C. Eldar, and A. Ribeiro.
Functional nonlinear sparse models.
<em>IEEE Trans. on Signal Process.</em>, 68[1]:2449–2463, 2020.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Peifer20s">12</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">M. Peifer, <strong>L. F. O. Chamon</strong>, S. Paternain, and A. Ribeiro.
Sparse multiresolution representations with adaptive kernels.
<em>IEEE Trans. on Signal Process.</em>, 68[1]:2031–2044, 2020.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Kalogerias20b">13</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">D. S. Kalogerias, <strong>L. F. O. Chamon</strong>, G. J. Pappas, and A. Ribeiro.
Better safe than sorry: <span class="bibtex-protected">R</span>isk-aware nonlinear <span class="bibtex-protected">B</span>ayesian estimation.
In <em>IEEE International Conference in Acoustic, Speech, and Signal Processing (ICASSP)</em>. 2020.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Arzani180">14</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">B. Arzani, S. Ciraci, <strong>L. F. O. Chamon</strong>, Y. Zhu, H. Liu, J. Padhye, B. T. Loo, and G. Outhred.
007: <span class="bibtex-protected">D</span>emocratically finding the cause of packet drops.
In <em>USENIX Symposium on Networked Systems Design and Implementation (NSDI)</em>, 419–435. 2018.</span>
</span>]</sup>,
  </span></li>
<li>and <strong>turn these insights into actions</strong>
<span style="color:#999;font-size:90%;">e.g.,
    using
    resource allocation<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Eisen19l">15</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">M. Eisen, C. Zhang, <strong>L. F. O. Chamon</strong>, D. D. Lee, and A. Ribeiro.
Learning optimal resource allocations in wireless systems.
<em>IEEE Trans. on Signal Process.</em>, 67[10]:2775–2790, 2019.</span>
</span>]</sup>,
    optimal control<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon20r">16</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong>, A. Amice, S. Paternain, and A. Ribeiro.
Resilient control: <span class="bibtex-protected">C</span>ompromising to adapt.
In <em>IEEE Control and Decision Conference</em>. 2020.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon20c">17</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong>, S. Paternain, and A. Ribeiro.
Counterfactual programming for optimal control.
In <em>Learning for Dynamics &amp; Control (L4DC)</em>. 2020.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon22a">18</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong>, A. Amice, and A. Ribeiro.
Approximately supermodular scheduling subject to matroid constraints.
<em>IEEE Trans. on Autom. Control.</em>, 67[3]:1384–1396, 2022.</span>
</span>]</sup>,
    and reinforcement learning<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Paternain19c">19</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">S. Paternain, <strong>L. F. O. Chamon</strong>, M. <span class="bibtex-protected">Calvo-Fullana</span>, and A. Ribeiro.
Constrained reinforcement learning has zero duality gap.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 7555–7565. 2019.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Paternain23s">20</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">S. Paternain, M. <span class="bibtex-protected">Calvo-Fullana</span>, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
Safe policies for reinforcement learning via primal-dual methods.
<em>IEEE Trans. on Autom. Control.</em>, 68[3]:1321–1336, 2023.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Calvo-Fullana24s">21</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">M. <span class="bibtex-protected">Calvo-Fullana</span>, S. Paternain, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
State augmented constrained reinforcement learning: <span class="bibtex-protected">O</span>vercoming the limitations of learning with rewards.
<em>IEEE Trans. on Autom. Control.</em>, 69[7]:4275–4290, 2024.</span>
</span>]</sup>
    algorithms.
  </span></li>
</ul>
<p>Our goal is to design AI systems that can learn and adapt with minimal human intervention while ensuring that they comply with rigorous operational requirements, such as
<strong>fairness</strong> of outcomes<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon20p">4</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong> and A. Ribeiro.
Probably approximately correct constrained learning.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>. 2020.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon23c">7</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong>, S. Paternain, M. <span class="bibtex-protected">Calvo-Fullana</span>, and A. Ribeiro.
Constrained learning with non-convex losses.
<em>IEEE Trans. on Inf. Theory</em>, 69[3]:1739–1760, 2023.</span>
</span>]</sup>,
<strong>robustness</strong> of predictions<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon20p">4</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong> and A. Ribeiro.
Probably approximately correct constrained learning.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>. 2020.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Robey21a">5</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">A. Robey*, <strong>L. F. O. Chamon</strong>*, G. J. Pappas, H. Hassani, and A. Ribeiro.
Adversarial robustness with semi-infinite constrained learning.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>. 2021.
<span class="bibtex-protected">(* equal contribution)</span>.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Robey22p">6</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">A. Robey, <strong>L. F. O. Chamon</strong>, G. J. Pappas, and H. Hassani.
Probabilistically robust learning: <span class="bibtex-protected">B</span>alancing average- and worst-case performance.
In <em>International Conference on Machine Learning (ICML)</em>. 2022.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon23c">7</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong>, S. Paternain, M. <span class="bibtex-protected">Calvo-Fullana</span>, and A. Ribeiro.
Constrained learning with non-convex losses.
<em>IEEE Trans. on Inf. Theory</em>, 69[3]:1739–1760, 2023.</span>
</span>]</sup>,
<strong>safety</strong> of behaviors<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Paternain19c">19</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">S. Paternain, <strong>L. F. O. Chamon</strong>, M. <span class="bibtex-protected">Calvo-Fullana</span>, and A. Ribeiro.
Constrained reinforcement learning has zero duality gap.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 7555–7565. 2019.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Paternain23s">20</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">S. Paternain, M. <span class="bibtex-protected">Calvo-Fullana</span>, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
Safe policies for reinforcement learning via primal-dual methods.
<em>IEEE Trans. on Autom. Control.</em>, 68[3]:1321–1336, 2023.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Calvo-Fullana24s">21</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">M. <span class="bibtex-protected">Calvo-Fullana</span>, S. Paternain, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
State augmented constrained reinforcement learning: <span class="bibtex-protected">O</span>vercoming the limitations of learning with rewards.
<em>IEEE Trans. on Autom. Control.</em>, 69[7]:4275–4290, 2024.</span>
</span>]</sup>,
and <strong>alignment</strong> with prior knowledge or desired features (e.g., smoothness<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Cervino23l">22</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">J. Cervino, <strong>L. F. O. Chamon</strong>, B. D. Haeffele, R. Vidal, and A. Ribeiro.
Learning globally smooth functions on manifolds.
In <em>International Conference on Machine Learning (ICML)</em>. 2023.</span>
</span>]</sup>, invariance<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Hounie23a">23</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">I. Hounie, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
Automatic data augmentation via invariance-constrained learning.
In <em>International Conference on Machine Learning (ICML)</em>. 2023.</span>
</span>]</sup>, scientific models<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Moro25s">24</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">V. Moro and <strong>L. F. O. Chamon</strong>.
Solving differential equations with constrained learning.
In <em>International Conference on Learning Representations (ICLR)</em>. 2025.</span>
</span>]</sup>).
Typically, these requirements are <em>induced</em> by aggregating violation metrics into the learning objective. To be effective, this objective-driven approach requires careful hyperparameter tuning and cross-validation, a time consuming and resource intensive (data, compute, environment) process.</p>
<p>To build trustworthy AI systems, <strong>my group treats requirements as integral parts of learning</strong> rather than afterthoughts or secondary objectives <strong>to build a requirement-driven learning paradigm.</strong>
We look at fundamental questions such as
"when is it possible to learn under requirements?" or "how much harder is it than vanilla learning?" (e.g., <sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon20p">4</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong> and A. Ribeiro.
Probably approximately correct constrained learning.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>. 2020.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon23c">7</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong>, S. Paternain, M. <span class="bibtex-protected">Calvo-Fullana</span>, and A. Ribeiro.
Constrained learning with non-convex losses.
<em>IEEE Trans. on Inf. Theory</em>, 69[3]:1739–1760, 2023.</span>
</span>]</sup>) as well as applications (e.g.,
image classification<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon20p">4</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong> and A. Ribeiro.
Probably approximately correct constrained learning.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>. 2020.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Robey21a">5</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">A. Robey*, <strong>L. F. O. Chamon</strong>*, G. J. Pappas, H. Hassani, and A. Ribeiro.
Adversarial robustness with semi-infinite constrained learning.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>. 2021.
<span class="bibtex-protected">(* equal contribution)</span>.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Robey22p">6</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">A. Robey, <strong>L. F. O. Chamon</strong>, G. J. Pappas, and H. Hassani.
Probabilistically robust learning: <span class="bibtex-protected">B</span>alancing average- and worst-case performance.
In <em>International Conference on Machine Learning (ICML)</em>. 2022.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Hounie23a">23</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">I. Hounie, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
Automatic data augmentation via invariance-constrained learning.
In <em>International Conference on Machine Learning (ICML)</em>. 2023.</span>
</span>]</sup>,
semi-supervised learning<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Cervino23l">22</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">J. Cervino, <strong>L. F. O. Chamon</strong>, B. D. Haeffele, R. Vidal, and A. Ribeiro.
Learning globally smooth functions on manifolds.
In <em>International Conference on Machine Learning (ICML)</em>. 2023.</span>
</span>]</sup>,
data-driven control<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Paternain19c">19</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">S. Paternain, <strong>L. F. O. Chamon</strong>, M. <span class="bibtex-protected">Calvo-Fullana</span>, and A. Ribeiro.
Constrained reinforcement learning has zero duality gap.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 7555–7565. 2019.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Paternain23s">20</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">S. Paternain, M. <span class="bibtex-protected">Calvo-Fullana</span>, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
Safe policies for reinforcement learning via primal-dual methods.
<em>IEEE Trans. on Autom. Control.</em>, 68[3]:1321–1336, 2023.</span>
</span>,<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Calvo-Fullana24s">21</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;">M. <span class="bibtex-protected">Calvo-Fullana</span>, S. Paternain, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
State augmented constrained reinforcement learning: <span class="bibtex-protected">O</span>vercoming the limitations of learning with rewards.
<em>IEEE Trans. on Autom. Control.</em>, 69[7]:4275–4290, 2024.</span>
</span>]</sup>).
This shift represents not just a technical advancement, but a fundamental reimagining of how we design and deploy intelligent systems. Ultimately, my vision is to take us away from a notion of <strong>artificial intelligence</strong> that is supposed to implicitly emerge from data to one of <strong>engineered intelligence</strong> that explicitly does what we want.</p>
<p>You can read more about this and other research projects below. If anything piques your interest,
feel free to reach out to me by <a href="https://www.luizchamon.com/pages/contact.html">email</a> or check the <a href="https://www.luizchamon.com/pages/prospective-members.html">prospective members</a> page.</p>
<h3 id="current-projects">Current projects</h3>
<ul>
<li><a href="#constrained-learning-theory">Constrained learning theory</a></li>
<li><a href="#semi-infinite-constrained-learning">Semi-infinite constrained learning</a></li>
<li><a href="#constrained-reinforcement-learning">Constrained reinforcement learning</a></li>
<li><a href="#graphon-neural-networks">Graph(on) neural networks</a></li>
</ul>
<h3 id="past-projects">Past projects</h3>
<ul>
<li><a href="#non-convex-functional-optimization">Non-convex functional optimization and sparsity</a></li>
<li><a href="#combinatorial-optimization-and-approximate-submodularity">Combinatorial optimization and approximate submodularity</a></li>
<li><a href="#combinations-of-adaptive-filters">Combinations of adaptive filters</a></li>
<li><a href="#aircraft-cabin-simulator">Real-sized aircraft cabin simulator</a></li>
</ul>
<p> </p>
<hr/>
<h1 id="people">People</h1>
<p>I am fortunate to work with many talented students, including</p>
<ul>
<li>Aneesh Barthakur, PhD student, U. Stuttgart, 2023–current</li>
<li>Andrei Manolache, PhD student (<em>main supervisor: Mathias Niepert</em>), U. Stuttgart, 2024–current</li>
<li>Tim Schneider, PhD student (<em>main supervisor: Steffen Staab</em>), U. Stuttgart, 2024–current</li>
<li>Vinh Tong, PhD student (<em>main supervisor: Mathias Niepert</em>), U. Stuttgart, 2024–current</li>
</ul>
<p>They are part of the many brilliant students I've had the pleasure of working with:</p>
<ul>
<li>Viggo Moro, U. Stuttgart, 2024 (<em>PhD student at the University of Oxford</em>)</li>
<li>Juan Elenter, PhD student (<em>main supervisor: Alejandro Ribeiro</em>), U. Pennsylvania, 2023–2024<br/>
  (<em>Spotify</em>)</li>
<li>Nadin Elsharbatly, Master student, U. Stuttgart, 2024</li>
<li>Ignacio Hounie, PhD student (<em>main supervisor: Alejandro Ribeiro</em>), U. Pennsylvania, 2021–2023</li>
<li>Luana Ruiz, PhD student (<em>main supervisor: Alejandro Ribeiro</em>), U. Pennsylvania, 2019–2022<br/>
  (<em>professor at Johns Hopkins university</em>)</li>
<li>Maria Peifer, PhD student (<em>main supervisor: Alejandro Ribeiro</em>), U. Pennsylvania, 2018–2021<br/>
  (<em>machine learning engineer at Comcast</em>)</li>
<li>Alexandre Amice,  bachelor student (<em>main supervisor: Alejandro Ribeiro</em>),
  U. Pennsylvania, 2018–2020<br/>
  (<em>PhD student at MIT</em>)</li>
</ul>
<p> </p>
<hr/>
<h1 id="current-projects_1">Current projects</h1>
<h2 id="constrained-learning-theory">Constrained learning theory</h2>
<div class="research-item">
<p class="research-icon"><img alt="Constrained learning theory icon" src="https://www.luizchamon.com/images/research/clt.png"/></p>
<p>Constrained learning uses the language of constrained optimization to tackle learning tasks
that involve statistical requirements. Its strength lies in combining the data-driven, model-free nature
of learning with the expressiveness of constrained programming. Doing so, however, also combines the statistical
challenges of the former with the computational complexity of the latter. Hence, it is natural
to wonder if constrained learning is feasible? If solving multiple learning problems simultaneously
does not compound their complexity? These are the type of questions that concern <strong>constrained learning theory</strong>.
We now know that constrained learning can solve problems that unconstrained learning cannot while often being
essentially as hard. In fact, it is usually possible to learn under constraints by solving only of unconstrained
problems. These results have already enabled many applications and opened up new theoretical questions on
the limits of this new learning task.</p>
</div>
<div class="collapsible">
<button type="button">Selected publications</button>
<div class="collapsible-content">
<ul class="pubs_list">
<li>
        V. Moro and <strong>L. F. O. Chamon</strong>.
Solving differential equations with constrained learning.
In <em>International Conference on Learning Representations (ICLR)</em>. 2025.
      <br>

        [ <a href="https://arxiv.org/abs/2410.22796">arXiv</a> ]




      </br></li>
<li>
<strong>L. F. O. Chamon</strong>, S. Paternain, M. <span class="bibtex-protected">Calvo-Fullana</span>, and A. Ribeiro.
Constrained learning with non-convex losses.
<em>IEEE Trans. on Inf. Theory</em>, 69[3]:1739–1760, 2023.
      <br>

        [ <a href="https://arxiv.org/abs/2103.05134">arXiv</a> ]




      </br></li>
<li>
<strong>L. F. O. Chamon</strong> and A. Ribeiro.
Probably approximately correct constrained learning.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>. 2020.
      <br>

        [ <a href="https://arxiv.org/abs/2006.05487">arXiv</a> ]


        [ <a href="https://www.luizchamon.com/pdf/chamon_neurips2020_poster.pdf">Poster</a> ]

      </br></li>
<li>
<strong>L. F. O. Chamon</strong>, S. Paternain, M. <span class="bibtex-protected">Calvo-Fullana</span>, and A. Ribeiro.
The empirical duality gap of constrained statistical learning.
In <em>IEEE International Conference in Acoustic, Speech, and Signal Processing (ICASSP)</em>. 2020.
          <strong>(Best student paper award)</strong>
<br>

        [ <a href="https://arxiv.org/abs/2002.05183">arXiv</a> ]
        [ <a href="https://youtu.be/0cl35wNAfiA">YouTube</a> ]
        [ <a href="https://www.luizchamon.com/pdf/chamon_icassp2020a_slides.pdf">Slides</a> ]


      </br></li>
<li>
        M. Eisen, C. Zhang, <strong>L. F. O. Chamon</strong>, D. D. Lee, and A. Ribeiro.
Learning optimal resource allocations in wireless systems.
<em>IEEE Trans. on Signal Process.</em>, 67[10]:2775–2790, 2019.
          <strong>(Top 50 most accessed articles in IEEE TSP: May, July, Sept, Oct 2019)</strong>
<br/>

        [ <a href="https://arxiv.org/abs/1807.08088">arXiv</a> ]




      </li>
</ul>
</div>
</div>
<p> </p>
<p> </p>
<h2 id="semi-infinite-constrained-learning">Semi-infinite constrained learning</h2>
<div class="research-item">
<p class="research-icon"><img alt="Semi-infinite constrained learning icon" src="https://www.luizchamon.com/images/research/sicl.png"/></p>
<p>Statistical requirements lie in a spectrum between <em>in expectation</em> and <em>almost surely</em>.
On the former end, learning is performed using empirical averages over the available data.
This is the case, for example, in wireless resource allocation, safe reinforcement learning,
and certain definitions of fairness. <strong>Semi-infinite constrained learning</strong> is primarily concerned
with problems on the other end of the spectrum, e.g., those involving min-max properties such as
robustness and invariance. For these almost sure requirements to hold, however, an infinite number
of constraints must be satisfied for each data point. Combining duality and hybrid
stochastic optimization–MCMC sampling algorithms yield a new approach to tackle to
these seemingly intractable problems. These developments have lead to new theoretical questions and
applications, such as smooth learning and probabilistic robustness, a property that lies strictly in the
interior of the expectation–almost sure spectrum.</p>
</div>
<div class="collapsible">
<button type="button">Selected publications</button>
<div class="collapsible-content">
<ul class="pubs_list">
<li>
        V. Moro and <strong>L. F. O. Chamon</strong>.
Solving differential equations with constrained learning.
In <em>International Conference on Learning Representations (ICLR)</em>. 2025.
      <br/>

        [ <a href="https://arxiv.org/abs/2410.22796">arXiv</a> ]




      </li>
<li>
        J. Cervino, <strong>L. F. O. Chamon</strong>, B. D. Haeffele, R. Vidal, and A. Ribeiro.
Learning globally smooth functions on manifolds.
In <em>International Conference on Machine Learning (ICML)</em>. 2023.
      <br/>

        [ <a href="https://arxiv.org/abs/2210.00301">arXiv</a> ]




      </li>
<li>
        I. Hounie, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
Automatic data augmentation via invariance-constrained learning.
In <em>International Conference on Machine Learning (ICML)</em>. 2023.
      <br/>

        [ <a href="https://arxiv.org/abs/2209.15031">arXiv</a> ]




      </li>
<li>
        A. Robey, <strong>L. F. O. Chamon</strong>, G. J. Pappas, and H. Hassani.
Probabilistically robust learning: <span class="bibtex-protected">B</span>alancing average- and worst-case performance.
In <em>International Conference on Machine Learning (ICML)</em>. 2022.
          <strong>(spotlight)</strong>
<br/>

        [ <a href="https://arxiv.org/abs/2202.01136">arXiv</a> ]




      </li>
<li>
        A. Robey*, <strong>L. F. O. Chamon</strong>*, G. J. Pappas, H. Hassani, and A. Ribeiro.
Adversarial robustness with semi-infinite constrained learning.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>. 2021.
<span class="bibtex-protected">(* equal contribution)</span>.
      <br/>

        [ <a href="https://arxiv.org/abs/2110.15767">arXiv</a> ]




      </li>
<li>
<strong>L. F. O. Chamon</strong> and A. Ribeiro.
Probably approximately correct constrained learning.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>. 2020.
      <br/>

        [ <a href="https://arxiv.org/abs/2006.05487">arXiv</a> ]


        [ <a href="https://www.luizchamon.com/pdf/chamon_neurips2020_poster.pdf">Poster</a> ]

      </li>
</ul>
</div>
</div>
<p> </p>
<p> </p>
<h2 id="constrained-reinforcement-learning">Constrained reinforcement learning</h2>
<div class="research-item">
<p class="research-icon"><img alt="Constrained reinforcement learning icon" src="https://www.luizchamon.com/images/research/crl.png"/></p>
<p>Constrained reinforcement learning (CRL) tackles constrained learning tasks arising in sequential decision making.
These interactive, dynamic settings lead to more subtle behaviors than their supervised learning counterpart.
While we can show that CRL is essentially as hard as vanilla RL, they turn out to be considerably problems.
In fact, there are very simple CRL tasks that cannot be solved using unconstrained RL (regardless of the choice
of reward). This means that typical primal-dual algorithms will also fail. Using a systematic state augmentation,
however, we can obtain a procedure that yields optimal, feasible trajectories without the need for randomization.
These developments lead to new challenges regarding multitask/multiobjective RL, the computational complexity of
CRL training, and the development of RL methods capable of handling non-stationary scenarios.</p>
</div>
<div class="collapsible">
<button type="button">Selected publications</button>
<div class="collapsible-content">
<ul class="pubs_list">
<li>
        M. <span class="bibtex-protected">Calvo-Fullana</span>, S. Paternain, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
State augmented constrained reinforcement learning: <span class="bibtex-protected">O</span>vercoming the limitations of learning with rewards.
<em>IEEE Trans. on Autom. Control.</em>, 69[7]:4275–4290, 2024.
      <br/>

        [ <a href="https://arxiv.org/abs/2102.11941">arXiv</a> ]




      </li>
<li>
        S. Paternain, M. <span class="bibtex-protected">Calvo-Fullana</span>, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
Safe policies for reinforcement learning via primal-dual methods.
<em>IEEE Trans. on Autom. Control.</em>, 68[3]:1321–1336, 2023.
      <br/>

        [ <a href="https://arxiv.org/abs/1911.09101">arXiv</a> ]




      </li>
<li>
        S. Paternain, <strong>L. F. O. Chamon</strong>, M. <span class="bibtex-protected">Calvo-Fullana</span>, and A. Ribeiro.
Constrained reinforcement learning has zero duality gap.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 7555–7565. 2019.
      <br/>

        [ <a href="https://arxiv.org/abs/1910.13393">arXiv</a> ]


        [ <a href="https://www.luizchamon.com/pdf/chamon_neurips2019_poster.pdf">Poster</a> ]

      </li>
</ul>
</div>
</div>
<p> </p>
<p> </p>
<h2 id="graphon-neural-networks">Graph(on) neural networks</h2>
<div class="research-item">
<p class="research-icon"><img alt="Graph/Graphon neural networks icon" src="https://www.luizchamon.com/images/research/gnn.png"/></p>
<p>Massive amounts of data in our increasingly interconnected world only make sense in
the context of the networks from which they arise, be them social networks, power grids,
IoT devices, or industry 4.0. Graph signal processing (GSP) and graph neural networks (GNNs)
grew out of the need to extract information from those network (graph) signals. These techniques,
however, are difficult to scale, hindering their use for large networks that can only be partially
observed or in non-stationary, dynamic settings. Yet, it seems reasonable that if two graphs are
"similar", then their graph Fourier transforms, graph filters, and GNNs should also be similar.
Formalizing this intuition is one of the motivations for developing the theory of
<strong>graphon signal processing</strong>. In fact, it has been used to show that GNNs are transferable
between graphs, i.e., that they can be trained on subgraphs to then be deployed on the full
network. These results raise fundamental questions on the limits of this transferability
as well as to what is the right graph similarity metric to characterize it.</p>
</div>
<div class="collapsible">
<button type="button">Selected publications</button>
<div class="collapsible-content">
<ul class="pubs_list">
<li>
        L. Ruiz, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
Transferability properties of graph neural networks.
<em>IEEE Trans. on Signal Process.</em>, 71:3474–3489, 2023.
      <br/>

        [ <a href="https://arxiv.org/abs/2112.04629">arXiv</a> ]
        [ <a href="https://www.youtube.com/watch?v=YOo19MLVimA">YouTube</a> ]



      </li>
<li>
        L. Ruiz, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
Graphon signal processing.
<em>IEEE Trans. on Signal Process.</em>, 69:4961–4976, 2021.
      <br/>

        [ <a href="https://arxiv.org/abs/2003.05030">arXiv</a> ]




      </li>
<li>
        L. Ruiz, <strong>L. F. O. Chamon</strong>, and A. Ribeiro.
Graphon neural networks and the transferability of graph neural networks.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>. 2020.
      <br/>

        [ <a href="https://arxiv.org/abs/2006.03548">arXiv</a> ]
        [ <a href="https://www.youtube.com/watch?v=YOo19MLVimA">YouTube</a> ]



      </li>
</ul>
</div>
</div>
<p> </p>
<p> </p>
<hr/>
<h1 id="past-projects_1">Past projects</h1>
<h2 id="non-convex-functional-optimization">Non-convex functional optimization</h2>
<div class="research-item">
<p class="research-icon"><img alt="Non-convex functional optimization icon" src="https://www.luizchamon.com/images/research/ncfo.png"/></p>
<p>Until 60 years ago, the tractability boundary in optimization separated <em>linear</em> from <em>nonlinear</em> programs.
Advances in convex analysis and barrier methods have since made it common place to hear <em>convex</em> used
as a synonym for <em>tractable</em> and <em>non-convex</em> as a synonym for <em>intractable</em>. Reality is naturally more subtle.
In fact, there are both computationally challenging <em>convex</em> programs—e.g., large-scale semi-definite
programming—and tractable <em>non-convex</em> ones—e.g., low-rank approximation. I am particularly
interested in a case of the latter that arises from the observation that
<strong>problems known to be intractable in finite dimensions often become tractable in infinite dimensions</strong>.
This means, for instance, that sparse regression is NP-hard in finite dimensions whereas its functional form is tractable.
This observation precludes the use of convex relaxations to tackle <em>off-the-grid compressive sensing</em> problems
and makes it possible to fit complex nonlinear models (from multi-resolution kernels to GPs), giving rise to new
statistical questions. It is also instrumental in the development of <a href="#constrained-learning-theory">constrained learning</a>.</p>
</div>
<div class="collapsible">
<button type="button">Selected publications</button>
<div class="collapsible-content">
<ul class="pubs_list">
<li>
<strong>L. F. O. Chamon</strong>, Y. C. Eldar, and A. Ribeiro.
Functional nonlinear sparse models.
<em>IEEE Trans. on Signal Process.</em>, 68[1]:2449–2463, 2020.
      <br/>

        [ <a href="https://arxiv.org/abs/1811.00577">arXiv</a> ]




      </li>
<li>
        M. Peifer, <strong>L. F. O. Chamon</strong>, S. Paternain, and A. Ribeiro.
Sparse multiresolution representations with adaptive kernels.
<em>IEEE Trans. on Signal Process.</em>, 68[1]:2031–2044, 2020.
      <br/>

        [ <a href="https://arxiv.org/abs/1905.02797">arXiv</a> ]




      </li>
<li>
<strong>L. F. O. Chamon</strong>, S. Paternain, and A. Ribeiro.
Learning <span class="bibtex-protected">G</span>aussian processes with <span class="bibtex-protected">B</span>ayesian posterior optimization.
In <em>Asilomar Conference on Signals, Systems and Computers</em>, 482–486. 2019.
      <br/>
        [ <a href="https://www.luizchamon.com/pdf/chamon_asilomar2019.pdf">PDF</a> ]


        [ <a href="https://www.luizchamon.com/pdf/chamon_asilomar2019_slides.pdf">Slides</a> ]


      </li>
</ul>
</div>
</div>
<p> </p>
<p> </p>
<h2 id="combinatorial-optimization-and-approximate-submodularity">Combinatorial optimization and approximate submodularity</h2>
<div class="research-item">
<p class="research-icon"><img alt="Combinatorial optimization and approximate submodularity icon" src="https://www.luizchamon.com/images/research/coaas.png"/></p>
<p>When the scale of the underlying system and/or technological constraints such as computation,
power, and communication, limit our capabilities, we are forced to choose a subset of the
available resources to use. These might be sensors for state estimation,
actuators for control, pixels for face recognition, or movie ratings to kick-start a
recommender systems. These selection problems are notoriously hard (often NP-hard in fact),
so that the best we can expect is to find an approximate solution. Greedy search is widely used in
this context due to its simplicity, iterative nature, and the fact that it is near-optimal
when the objective has a "diminishing returns" property known as <em>submodularity</em>. Yet, despite 
the empirical evidence for its effectiveness in the above problems, <strong>none of them are <em>submodular</em>.</strong>
In fact, quadratic costs generally only display "diminishing returns" under stringent conditions.
What this research has shown is that while the MSE and the LQR objective are not submodular,
they are not far from it, enjoying similar near-optimal properties. While many notions of
approximate submodularity had been investigated before, these were the first computable,
<em>a priori</em> guarantees for these problems, precluding the use of submodular surrogates (e.g., logdet) or
convex relaxations.</p>
</div>
<div class="collapsible">
<button type="button">Selected publications</button>
<div class="collapsible-content">
<ul class="pubs_list">
<li>
<strong>L. F. O. Chamon</strong>, A. Amice, and A. Ribeiro.
Approximately supermodular scheduling subject to matroid constraints.
<em>IEEE Trans. on Autom. Control.</em>, 67[3]:1384–1396, 2022.
      <br/>

        [ <a href="https://arxiv.org/abs/2003.08841">arXiv</a> ]




      </li>
<li>
<strong>L. F. O. Chamon</strong>, G. J. Pappas, and A. Ribeiro.
Approximate supermodularity of <span class="bibtex-protected">K</span>alman filter sensor selection.
<em>IEEE Trans. on Autom. Control.</em>, 66[1]:49–63, 2021.
      <br/>

        [ <a href="https://arxiv.org/abs/1912.03799">arXiv</a> ]




      </li>
<li>
<strong>L. F. O. Chamon</strong>, A. Amice, and A. Ribeiro.
Matroid-constrained approximately supermodular optimization for near-optimal actuator scheduling.
In <em>IEEE Control and Decision Conference</em>, 3391–3398. 2019.
      <br/>
        [ <a href="https://www.luizchamon.com/pdf/chamon_cdc2019a.pdf">PDF</a> ]


        [ <a href="https://www.luizchamon.com/pdf/chamon_cdc2019a_slides.pdf">Slides</a> ]


      </li>
<li>
<strong>L. F. O. Chamon</strong> and A. Ribeiro.
Greedy sampling of graph signals.
<em>IEEE Trans. on Signal Process.</em>, 66[1]:34–47, 2018.
      <br/>

        [ <a href="https://arxiv.org/abs/1704.01223">arXiv</a> ]




      </li>
<li>
<strong>L. F. O. Chamon</strong> and A. Ribeiro.
Approximate supermodularity bounds for experimental design.
In <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 5403–5412. 2017.
      <br/>

        [ <a href="https://arxiv.org/abs/1711.01501">arXiv</a> ]


        [ <a href="https://www.luizchamon.com/pdf/chamon_neurips2017_poster.pdf">Poster</a> ]

      </li>
<li>
<strong>L. F. O. Chamon</strong> and A. Ribeiro.
Near-optimality of greedy set selection in the sampling of graph signals.
In <em>IEEE Global Conference on Signal and Information Processing (GlobalSip)</em>, 1265–1269. 2016.
      <br/>
        [ <a href="https://www.luizchamon.com/pdf/chamon_globalsip2016.pdf">PDF</a> ]


        [ <a href="https://www.luizchamon.com/pdf/chamon_globalsip2016_slides.pdf">Slides</a> ]


      </li>
</ul>
</div>
</div>
<p> </p>
<p> </p>
<h2 id="combinations-of-adaptive-filters">Combinations of adaptive filters</h2>
<div class="research-item">
<p class="research-icon"><img alt="Combinations of adaptive filters icon" src="https://www.luizchamon.com/images/research/caf.png"/></p>
<p>As most stochastic optimization algorithms, adaptive filters suffer from trade-offs
that can hinder their use in practice. Larger step sizes, for example, lead to faster
convergence and better tracking, but also worse steady-state errors. Combinations of
adaptive filters were proposed to address such compromises by mixing the output of a
fast filter with that of an accurate one and adjusting that mixture depending on which
filter is performing best. How these outputs are combined has a large influence in the
resulting performance, so this research program set out to determine
<strong>what is the best way to combine adaptive filters.</strong> To do so, it developed an
algebra to describe combinations of adaptive filters using message passing graphs
and used it to design and analyze a myriad of combination topologies tackling a
diversity of new applications. In one instance, a combination of simple adaptive
filters was used to <strong>reduce the complexity of adaptive algorithms</strong> by
outperforming complex, Newton-type adaptive methods at roughly 30 times
lower computational complexity.</p>
</div>
<div class="collapsible">
<button type="button">Selected publications</button>
<div class="collapsible-content">
<ul class="pubs_list">
<li>
        C. G. Lopes, V. H. Nascimento, and <strong>L. F. O. Chamon</strong>.
Distributed universal adaptive networks.
<em>IEEE Trans. on Signal Process.</em>, 71:1817–1832, 2023.
      <br/>

        [ <a href="https://arxiv.org/abs/2307.05746">arXiv</a> ]




      </li>
<li>
<strong>L. F. O. Chamon</strong> and C. G. Lopes.
Combination of <span class="bibtex-protected">LMS</span> adaptive filters with coefficients feedback.
2016.
      <br/>

        [ <a href="https://arxiv.org/abs/1608.03248">arXiv</a> ]




      </li>
<li>
<strong>L. F. O. Chamon</strong> and C. G. Lopes.
There's plenty of room at the bottom: <span class="bibtex-protected">I</span>ncremental combinations of sign-error <span class="bibtex-protected">LMS</span> filters.
In <em>IEEE International Conference in Acoustic, Speech, and Signal Processing (ICASSP)</em>, 7248–7252. 2014.
      <br/>
        [ <a href="https://www.luizchamon.com/pdf/chamon_icassp2014.pdf">PDF</a> ]



        [ <a href="https://www.luizchamon.com/pdf/chamon_icassp2014_poster.pdf">Poster</a> ]

      </li>
<li>
<strong>L. F. O. Chamon</strong> and C. G. Lopes.
Transient performance of an incremental combination of <span class="bibtex-protected">LMS</span> filters.
In <em>European Signal Processing Conference (EUSIPCO)</em>, 7298–7302. 2013.
      <br/>
        [ <a href="https://www.luizchamon.com/pdf/chamon_eusipco2013.pdf">PDF</a> ]



        [ <a href="https://www.luizchamon.com/pdf/chamon_eusipco2013_poster.pdf">Poster</a> ]

      </li>
<li>
<strong>L. F. O. Chamon</strong>, H. F. Ferro, and C. G. Lopes.
A data reusage algorithm based on incremental combination of <span class="bibtex-protected">LMS</span> filters.
In <em>Asilomar Conference on Signals, Systems and Computers</em>, 406–410. 2012.
      <br/>
        [ <a href="https://www.luizchamon.com/pdf/chamon_asilomar2012.pdf">PDF</a> ]



        [ <a href="https://www.luizchamon.com/pdf/chamon_asilomar2012_poster.pdf">Poster</a> ]

      </li>
<li>
<strong>L. F. O. Chamon</strong>, W. B. Lopes, and C. G. Lopes.
Combination of adaptive filters with coefficients feedback.
In <em>IEEE International Conference in Acoustic, Speech, and Signal Processing (ICASSP)</em>, 3785–3788. 2012.
      <br/>
        [ <a href="https://www.luizchamon.com/pdf/chamon_icassp2012.pdf">PDF</a> ]



        [ <a href="https://www.luizchamon.com/pdf/chamon_icassp2012_poster.pdf">Poster</a> ]

      </li>
</ul>
</div>
</div>
<p> </p>
<p> </p>
<h2 id="aircraft-cabin-simulator">Aircraft cabin simulator</h2>
<p><img alt="Figure 1" src="https://www.luizchamon.com/images/research/sarava_luz.jpg"/></p>
<p>Air passenger traffic has grown enormously in the last few decades. This increase in competition
has made aircraft carriers and manufacturers aware of the need to find new ways to attract
customers, inevitably turning to the comfort factor. Although studies on automotive comfort are
abundant, those on aircraft environments are scarce, partly due to the difficulties in running
experiments (costs, risks...). In order to address these issues, a real-sized aircraft cabin
simulator capable of controlling variables such as sound, vibration, temperature, air flow,
pressure, and lighting, was built at the University of São Paulo in collaboration with EMBRAER.</p>
<p>I co-designed and built the vibro-acoustic reproduction system, composed of more than
20 loudspeakers and 30 shakers. Using new MIMO equalization methods<sup style="font-size: 80%; white-space: nowrap;">[<span class="tooltip">
<a href="https://www.luizchamon.com/pages/publications.html#Chamon11a">25</a>
<span class="tooltiptext" style="white-space: normal;font-size: 12.9px; color: #515151;"><strong>L. F. O. Chamon</strong>, G. S. Quiqueto, S. R. Bistafa, and V. H. Nascimento.
An <span class="bibtex-protected">SVD</span>-based <span class="bibtex-protected">MIMO</span> equalizer applied to the auralization of aircraft noise in a cabin simulator.
In <em>18th International Congress on Sound and Vibration (ICSV)</em>. 2011.</span>
</span>]</sup>,
we were able to precisely simulate the noise patterns of dozens of aircraft, including take-off
and landing. From the control room, it is possible to monitor the environment inside the simulator
using microphones and accelerometers installed on each seat. After half a decade of work,
this project culminated in more than 60 simulated flights involved over 1000 people.
I was responsible for the statistical analysis of the results to understand the interplay
between variables and passenger comfort. This simulator is still being used in collaborations
between the University of São Paulo and aeronautic industries.</p>
<div class="collapsible">
<button type="button">Selected publications</button>
<div class="collapsible-content">
<ul class="pubs_list">
<li>
        R. F. Bittencourt, <strong>L. F. O. Chamon</strong>, S. Futatsugui, J. I. Yanagihara, and S. N. Y. Gerges.
Preliminary results on the modeling of aircraft vibroacoustic comfort.
In <em>INTERNOISE</em>. 2012.
      <br/>
        [ <a href="https://www.luizchamon.com/pdf/chamon_internoise2012.pdf">PDF</a> ]


        [ <a href="https://www.luizchamon.com/pdf/chamon_internoise2012_slides.pdf">Slides</a> ]


      </li>
<li>
<strong>L. F. O. Chamon</strong>, G. S. Quiqueto, S. R. Bistafa, and V. H. Nascimento.
An <span class="bibtex-protected">SVD</span>-based <span class="bibtex-protected">MIMO</span> equalizer applied to the auralization of aircraft noise in a cabin simulator.
In <em>18th International Congress on Sound and Vibration (ICSV)</em>. 2011.
      <br/>
        [ <a href="https://www.luizchamon.com/pdf/chamon_icsv2011.pdf">PDF</a> ]


        [ <a href="https://www.luizchamon.com/pdf/chamon_icsv2011_slides.pdf">Slides</a> ]


      </li>
<li>
<strong>L. F. O. Chamon</strong>, G. S. Quiqueto, and S. R. Bistafa.
The application of the <span class="bibtex-protected">S</span>ingular <span class="bibtex-protected">V</span>alue <span class="bibtex-protected">D</span>ecomposition for the decoupling of the vibratory reproduction system of an aircraft cabin simulator.
In <em>II SAE Brazil International Noise and Vibration Congress</em>. 2010.
      <br/>
        [ <a href="https://www.luizchamon.com/pdf/chamon_sae2010.pdf">PDF</a> ]





      </li>
</ul>
</div>
</div>
<p> </p>
<p><img alt="Figure 2" src="https://www.luizchamon.com/images/research/sarava_mics.png"/></p>
<script>
var coll = document.querySelectorAll(".collapsible button");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.parentNode.classList.toggle("active");
    var content = this.parentNode.getElementsByClassName("collapsible-content")[0];
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
</div>
<p class="copyright">© 2020–2025. All rights reserved.</p>
</body>
<script type="text/javascript">
  const menuBtn = document.getElementById('mobile-menu-open');
  const menuBtnAlt = document.getElementById('mobile-menu-open-alt');
  
  menuBtn.addEventListener('click', function (e) {
    e.preventDefault();
    e.stopPropagation();
    document.getElementById('mobile-navigation').classList.add('open');

    document.getElementById('backdrop').addEventListener('click', closeMenu);
    document.getElementById('menu-close').addEventListener('click', closeMenu);
  });

  menuBtnAlt.addEventListener('click', function (e) {
    e.preventDefault();
    e.stopPropagation();
    document.getElementById('mobile-navigation').classList.add('open');

    document.getElementById('backdrop').addEventListener('click', closeMenu);
    document.getElementById('menu-close').addEventListener('click', closeMenu);
  });

  function closeMenu(e) {
    e.preventDefault();
    e.stopPropagation();

    if ( document.getElementById('mobile-navigation').classList.contains('open') ) {
        document.getElementById('mobile-navigation').classList.remove('open');
        document.getElementById('backdrop').removeEventListener('click', closeMenu);
        document.getElementById('menu-close').removeEventListener('click', closeMenu);
    }
  }
</script>
<script crossorigin="anonymous" src="https://kit.fontawesome.com/19c5dfafcc.js"></script>
</html>